experiment:
  name: "bert_fine_tuning_v1"
  output_dir: "experiments/bert_fine_tuning_v1"
  seed: 42
  tags: ["bert", "classification", "production"]

model:
  type: "transformer"
  architecture:
    num_layers: 12
    hidden_size: 768
    num_attention_heads: 12
    intermediate_size: 3072
    dropout: 0.1
    activation: "gelu"
  pretrained_path: "/models/bert-base-uncased"
  
data:
  train_path: "data/train"
  val_path: "data/val"
  test_path: "data/test"
  source: "database"
  connection:
    type: "postgresql"
    host: "localhost"
    port: 5432
    database: "ml_experiments"
  query: |
    SELECT text, label 
    FROM training_data 
    WHERE created_at > '2024-01-01'
    AND quality_score > 0.8
  preprocessing:
    tokenizer: "bert-base-uncased"
    max_length: 512
    padding: true
    truncation: true
  batch_size: 32
  
training:
  optimizer:
    type: "adamw"
    lr: 2e-5
    weight_decay: 0.01
    warmup_steps: 1000
  scheduler:
    type: "linear"
    num_training_steps: 10000
  epochs: 3
  gradient_accumulation_steps: 4
  
logging:
  wandb:
    project: "my-ml-project"
    entity: "my-team"
  tensorboard:
    log_dir: "./logs"